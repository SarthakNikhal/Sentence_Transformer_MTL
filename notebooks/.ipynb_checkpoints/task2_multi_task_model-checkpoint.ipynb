{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab3dd1a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch.nn as nn\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb9c26a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Fake sentences\n",
    "sentences = [\n",
    "    \"Artificial Intelligence is evolving rapidly.\",  # Tech (Task A), Positive (Task B)\n",
    "    \"The government passed a new healthcare law.\",    # Politics (Task A), Neutral (Task B)\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "class SentenceTransformer(nn.Module):\n",
    "    def __init__(self, model_name='distilbert-base-uncased'):\n",
    "        super(SentenceTransformer, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Mean pooling\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size())\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "class MultiTaskSentenceTransformer(nn.Module):\n",
    "    def __init__(self, model_name='distilbert-base-uncased', \n",
    "                 num_classes_task_a=3, num_classes_task_b=3):\n",
    "        super(MultiTaskSentenceTransformer, self).__init__()\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        hidden_size = 768  # DistilBERT hidden size\n",
    "\n",
    "        # Task-specific heads\n",
    "        self.classification_head = nn.Linear(hidden_size, num_classes_task_a)\n",
    "        self.sentiment_head = nn.Linear(hidden_size, num_classes_task_b)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, task='A'):\n",
    "        embeddings = self.encoder(input_ids, attention_mask)\n",
    "        if task == 'A':\n",
    "            return self.classification_head(embeddings)\n",
    "        elif task == 'B':\n",
    "            return self.sentiment_head(embeddings)\n",
    "        else:\n",
    "            raise ValueError(\"Task must be 'A' or 'B'\")\n",
    "\n",
    "# Initialize model\n",
    "mtl_model = MultiTaskSentenceTransformer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d9bc98",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task A (Classification) Logits:\n",
      " tensor([[ 0.0254, -0.3884,  0.2180],\n",
      "        [ 0.1403, -0.1674,  0.0340]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Forward pass for Task A (Classification)\n",
    "logits_task_a = mtl_model(inputs['input_ids'], inputs['attention_mask'], task='A')\n",
    "print(\"Task A (Classification) Logits:\\n\", logits_task_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f1eafa4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task B (Sentiment) Logits:\n",
      " tensor([[ 0.2342,  0.1590, -0.2486],\n",
      "        [ 0.1502,  0.1129,  0.1144]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Forward pass for Task B (Sentiment Analysis)\n",
    "logits_task_b = mtl_model(inputs['input_ids'], inputs['attention_mask'], task='B')\n",
    "print(\"Task B (Sentiment) Logits:\\n\", logits_task_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84987aee-0ce4-47d4-a3b9-e69a55c3b8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
